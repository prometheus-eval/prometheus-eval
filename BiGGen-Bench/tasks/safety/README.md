10 tasks related to safely deploying LLMs. For detailed information, please refer to this [survey paper](https://arxiv.org/abs/2305.11391).

Name | Description | Data Annotator (Group A) | Data Reviewer (Group A) | Data Annotator (Group B) | Related paper 
---- | ----------- | -------------------- | -------------------- | --------------- | --------------- |
[honesty](honesty/) | Tasks that provide synthetic information as additional input and probes the model to state whether it knows or does not know a queried information. | Yejin Cho | Dongkeun Yoon, Sue Hyun Park | ? | [link1](https://arxiv.org/abs/2312.07000) [link2](https://arxiv.org/abs/2311.09677) |
[moral_belief](moral_belief/) | Tasks necessitates possessing an opinion in a moral dilemma situation | Yejin Cho | Dongkeun Yoon, Sue Hyun Park | ? | [link](https://arxiv.org/abs/2309.13356) |
[knowledge_unlearning](knowledge_unlearning/) | Tasks that require to unlearn certain concepts related to privacy issues by providing a set of in-context demonstrations | Yejin Cho | Dongkeun Yoon, Sue Hyun Park | ? | [link1](https://arxiv.org/abs/2310.07579), [link2](https://arxiv.org/abs/2210.01504)
[mentioning_potential_harm](mentioning_potential_harm/) | Tasks that require explaining a list of items. The main objective is to check whether at least one of the items mentions about the potential harm when executing an action. | Yejin Cho | Dongkeun Yoon, Sue Hyun Park | ? | [link](https://arxiv.org/abs/2310.08491) |
[determine_what_is_wrong](determine_what_is_wrong/) | Tasks that require to to explain what is controversial about a given text. The model should be able to state historical backgrounds, provenance, and examples of why it is toxic to use such term. | Yejin Cho | Dongkeun Yoon, Sue Hyun Park | ? | [link1](https://arxiv.org/abs/2310.07579), [link2](https://arxiv.org/abs/2210.01504)
[keep_a_secret](keep_a_secret/) | Tasks that prompt language models to generate certain informations. The objective of the task is to not sensitive information within its responses | Noah Lee | Haebin Shin, Hyungjoo Chae | ? | [link1](https://arxiv.org/abs/2311.16119) [link2](https://arxiv.org/abs/2310.17884) |
[safety_alignment](safety_alignment/) | Tasks that red team language models to generate toxic content by providing a set of in-context demonstrations | Noah Lee | Haebin Shin, Hyungjoo Chae | ? | [link1](https://arxiv.org/abs/2310.02949) |
[detect_ai_generation](detect_ai_generation/) | Tasks that require to detect whether a text is generated from AI or human | Noah Lee | Haebin Shin, Hyungjoo Chae | ? | [link1](https://arxiv.org/abs/2301.10226), [link2](https://arxiv.org/abs/2301.11305)
[if_else_statements](if_else_statements/) | Tasks that require to generate a piece of code that divides priority in a given situation | Noah Lee | Haebin Shin, Hyungjoo Chae | ? | [link1](https://arxiv.org/abs/2209.07858) [link2](https://huggingface.co/blog/red-teaming) |
[red_teaming_prompts](red_teaming_prompts/) | Tasks that test the general robusteness to red-team attempts. | Noah Lee | Haebin Shin, Hyungjoo Chae | ? | [link1](https://arxiv.org/abs/2202.03286) [link2](https://arxiv.org/abs/2401.06373)
